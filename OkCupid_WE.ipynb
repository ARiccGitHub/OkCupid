{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **OkCupid**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My Codecademy OkCupid Machine Learning Portfolio Project from the Data Scientist Path.<br>\n",
    "<br>\n",
    "I divided the project in two sections;\n",
    "- OKCupid ID - Data Investigation \n",
    "    - Provided data investigation\n",
    "    - NLP text pre-processing\n",
    "- OkCupid TF-IDF - NLP Term Frequency–Inverse Document Frequency (TF-IDF) \n",
    "    - TF-IDF scores computation\n",
    "    - TF-IDF terms results analysis\n",
    "- OkCupid WB - Word Embeddings (this section)\n",
    "    - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### + Project Goal\n",
    "Using data from [OKCupid](https://www.okcupid.com/), an app that focuses on using multiple choice and short answers to match users, formulate questions and implement machine learning techniques to answer those questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### + Overview\n",
    "In recent years, there has been a massive rise in the usage of dating apps to find love. Many of these apps use sophisticated data science techniques to recommend possible matches to users and to optimize the user experience. These apps give us access to a wealth of information that we’ve never had before about how different people experience romance.\n",
    "\n",
    "In this portfolio project, I analyze data from OKCupid, formulate questions and implement machine learning techniques to answer the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### + Project Requirements\n",
    "Be familiar with:\n",
    "\n",
    "- Python3\n",
    "- Machine Learning: \n",
    "     - Unsupervised Learning\n",
    "     - Supervised Learning\n",
    "     - Natural Language Processing\n",
    "- The Python Libraries:\n",
    "    - re\n",
    "    - gc\n",
    "    - Pandas\n",
    "    - NumPy\n",
    "    - Matplotlib\n",
    "    - Collections\n",
    "    - Sklearn\n",
    "    - NLT\n",
    "    - Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  + OkCupid DI project memory management\n",
    "This project requires jupyter notebook to use the python 64bit version, the 32bit version will generate a [MemoryError](https://docs.python.org/3/library/exceptions.html?highlight=memoryerror#MemoryError) when manipulating the provided data.<br>\n",
    "If you want to use this project code lines and you are unsure of which python bit version your Jupyter Notebook uses, you can enter the following code lines in your notebook:\n",
    "```python\n",
    "import struct\n",
    "print(struct.calcsize(\"P\") * 8)\n",
    "```\n",
    "You may also consider, increasing your Jupyter Notebook defaulted maximum memory buffer value.<br>\n",
    "The Jupyter Notebook maximum memory buffer is defaulted to 536,870,912 bytes.<br>\n",
    "[How to increase Jupyter notebook Memory limit?](https://stackoverflow.com/questions/57948003/how-to-increase-jupyter-notebook-memory-limit)<br>\n",
    "[Configure (Jupyter notebook) file and command line options](https://jupyter-notebook.readthedocs.io/en/stable/config.html#config-file-and-command-line-options)<br>\n",
    "\n",
    "I increased my Jupyter Notebook maximum memory buffer value to 8GB, my PC has 16GB of RAM.<br>\n",
    "When using the full size of the provided data, you need a minimum of 3GB of free RAM to run this project.<br>\n",
    "If RAM is an issue, you may consider using a sample of the provided data instead of the entire size of the provided data.<br>\n",
    "You can also utilize:\n",
    "- [Garbage Collector interface](https://docs.python.org/3/library/gc.html) library, [Python Garbage Collection: What It Is and How It Works](https://stackify.com/python-garbage-collection/) \n",
    "- And the `del` python function, [What does “del” do exactly?](https://stackoverflow.com/questions/21053380/what-does-del-do-exactly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### + Links\n",
    "My Project Blog Presentation\n",
    "\n",
    "[Project GitHub](https://github.com/ARiccGitHub/OkCupid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **OkCupid WE**\n",
    "### **Word Embeddings**\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "[Word embeddings](https://machinelearningmastery.com/what-are-word-embeddings/#:~:text=A%20word%20embedding%20is%20a,challenging%20natural%20language%20processing%20problems.) are a type of word representation that allows words with similar meaning to have a similar representation. In NLP words are often represented as numeric vectors, the algorithms used to vectorize words are referred to as \"words to vectors\"([word2vec](https://en.wikipedia.org/wiki/Word2vec)).\n",
    "\n",
    "The idea behind word embeddings is a theory known as the distributional hypothesis. This hypothesis states that words that co-occur in the same contexts tend to have similar meanings. Word2Vec is a shallow neural network model that can build word embeddings using either continuous bag-of-words or continuous skip-grams.<br>\n",
    "The word2vec method that I use to create word embeddings is based on continuous skip-grams. Skip-grams function similarly to n-grams, except instead of looking at groupings of n-consecutive words in a text, we can look at sequences of words that are separated by some specified distance between them.\n",
    "<br>\n",
    "<br>\n",
    "In this section I answer the question: In the contest of the OkCupid essays, which terms have similar meanings in the essay features by essays, by category and mix-categories?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **▪ Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation tool\n",
    "import pandas as pd\n",
    "# Regex\n",
    "import re\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "# word2vec model library, Word Emmbeddings\n",
    "import gensim\n",
    "# Garbage Collector interface - https://docs.python.org/3/library/gc.html\n",
    "import gc\n",
    "gc.set_threshold(100, 10, 10)\n",
    "#---- My Local python files\n",
    "import project_library as pjl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **▪ Text Pre-processing**\n",
    "<br>\n",
    "\n",
    "The essays features and categories descriptions:<br>\n",
    "<br>\n",
    "Features:\n",
    "\n",
    "| | |\n",
    "| --- | :-- |\n",
    "| essay0: | My Self summary|\n",
    "| essay1: | What I’m doing with my life|\n",
    "| essay2: | I’m really good at|\n",
    "| essay3: | The first thing people usually notice about me|\n",
    "| essay4: | Favorite books, movies, show, music, and food|\n",
    "| essay5: | The six things I could never do without|\n",
    "| essay6: | I spend a lot of time thinking about|\n",
    "| essay7: | On a typical Friday night I am|\n",
    "| essay8: | The most private thing I am willing to admit|\n",
    "| essay9: | You should message me if...|\n",
    "\n",
    "<br>\n",
    "Categories\n",
    "\n",
    "| age | sex | orientation | ethnicity | pets |\n",
    "|:-:|:-:|:-:|:-:|:-:|\n",
    "| under 25 | female | straight | white | no-answer |\n",
    "| 25 to 35 | male | gay | none_white | likes dogs and likes cats |\n",
    "| 35 to 45 | | bisexual | | likes dogs and has cats |\n",
    "| over 45 | | | | has dogs and likes cats |\n",
    "| | | | | likes dogs and dislikes cats |\n",
    "| | | | | has dogs and has cats |\n",
    "| | | | | has dogs |\n",
    "| | | | | has cats |\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "The essays are tokenize by terms and sentences, by essay features and categories.\n",
    "The essays text pre-processing was completed in the <a href=\"OkCupid_DI.ipynb\">OkCupid DI<a> section.\n",
    "<br>  \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### + Loading the pre-processed data\n",
    "\n",
    "<br>\n",
    "\n",
    "For this project, I use the the [pandas.HDFStore](https://www.kite.com/python/docs/pandas.HDFStore) class to store my DataFrames.\n",
    "\n",
    ">[HDF5](https://www.neonscience.org/resources/learning-hub/tutorials/about-hdf5#:~:text=The%20Hierarchical%20Data%20Format%20version,with%20files%20on%20your%20computer.) is a format designed to store large numerical arrays of homogenous type. It came particularly handy when you need to organize your data models in a hierarchical fashion and you also need a fast way to retrieve the data. Pandas implements a quick and intuitive interface for this format and in this post will shortly introduce how it works. - [The Glowing Python](https://glowingpython.blogspot.com/2014/08/quick-hdf5-with-pandas.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opens, in append mode, pre-processed data\n",
    "profiles_nlp = pd.HDFStore('data/profiles_nlp.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **▪ Vocabularies of Terms**\n",
    "<br>\n",
    "In this section, in the contest of the OkCupid essays, I create vocabularies of terms relative to each essay feature by all categories, by category and mix-categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essay feature names \n",
    "essay_names = ['essay0', 'essay1', 'essay2', 'essay3', 'essay4', 'essay5', 'essay6', 'essay7', 'essay8', 'essay9']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### + Vocabularies of terms by  'age_bracket'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = profiles_nlp.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = gensim.models.Word2Vec(profiles_nlp['essay0_sentence_words_all'], size=96, window=5, min_count=1, workers=2, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 0.8628436326980591),\n",
       " ('puppy', 0.8199577927589417),\n",
       " ('pet', 0.8032441735267639),\n",
       " ('pup', 0.792530357837677),\n",
       " ('animal', 0.7825057506561279),\n",
       " ('rescue', 0.7672238349914551),\n",
       " ('kitty', 0.7649505138397217),\n",
       " ('kitten', 0.7556507587432861),\n",
       " ('critter', 0.7508859038352966),\n",
       " ('parrot', 0.7450723052024841),\n",
       " ('leash', 0.7347059845924377),\n",
       " ('furry', 0.7338184118270874),\n",
       " ('pug', 0.7301511764526367),\n",
       " ('husky', 0.7284238934516907),\n",
       " ('max', 0.720120906829834),\n",
       " ('doggy', 0.7158674001693726),\n",
       " ('chihuahua', 0.7157939672470093),\n",
       " ('labrador', 0.712922990322113),\n",
       " ('shelter', 0.7117931842803955),\n",
       " ('wag', 0.7111159563064575)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings.wv.most_similar('dog', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word_e:\n",
    "    # Global variables\n",
    "    global profiles_nlp\n",
    "    global essay_names\n",
    "    # -------------------------------------------------------------------------------------------------------------------------------- Special methods\n",
    "    # ----------------------------------------------------------------------------------- Initialization\n",
    "    def __init__(self, cat1=[], cat2=[], n=15):\n",
    "        '''\n",
    "        Takes the arguments:\n",
    "            - cat1, list data type, defaulted to [], essay category elements list\n",
    "            - cat2, list data type, defaulted to [], essay category elements list\n",
    "            - n, integer data type, defaulted to `'15'`, (n terms/the highest TF-IDF score)\n",
    "        Checks number of category\n",
    "        Calls the method tfidf_compute()\n",
    "        '''\n",
    "        self.cat1 = cat1\n",
    "        self.cat2 = cat2\n",
    "        self.n = n\n",
    "        # Empty DataFrame lists\n",
    "        self.tfidf_scores = []\n",
    "        self.tfidf_terms = []\n",
    "        # No category entered error\n",
    "        if cat1 == [] and cat2 == []:\n",
    "            print('------ ERROR ------\\ncat1 argumnet missing')\n",
    "            return\n",
    "        # One category entered\n",
    "        if cat2 == []:\n",
    "            for c1 in cat1:\n",
    "                scores, terms = self.__tfidf_compute(f'preprocessed_essays_{c1}')\n",
    "                self.tfidf_scores.append(scores)\n",
    "                self.tfidf_terms.append(terms)\n",
    "        # Two category entered\n",
    "        else:\n",
    "            for c1 in cat1:\n",
    "                for c2 in cat2:\n",
    "                    scores, terms = self.__tfidf_compute(f'preprocessed_essays_{c1}_{c2}')\n",
    "                    self.tfidf_scores.append(scores)\n",
    "                    self.tfidf_terms.append(terms)\n",
    "    # ----------------------------------------------------------------------------------- Representation\n",
    "    def __repr__(self):\n",
    "        if self.cat2 == []:\n",
    "            return f'Tfidf(cat1={self.cat1}, n={self.n})'\n",
    "        return f'Tfidf(cat1={self.cat1}, cat2={self.cat2}, n={self.n})'        \n",
    "    # ----------------------------------------------------------------------------------- Class instance description\n",
    "    def __str__(self):\n",
    "        if self.cat2 == []:\n",
    "            return f'The class computes, for each essay feature entered categories:\\n     - {self.cat1}\\nAlso computes the n={self.n} highest terms TF-IDF score, and sums the scores by terms.'\n",
    "        return f'The class computes, for each essay feature entered categories:\\n     - {self.cat1}\\n     - {self.cat2}\\n\\nAlso computes the n={self.n} highest terms TF-IDF score, and sums the scores by terms.'\n",
    "    # -------------------------------------------------------------------------------------------------------------------------------- Private methods\n",
    "    # ----------------------------------------------------------------------------------- TF-IDF computing \n",
    "    def __word_e_compute(self, cat):\n",
    "        '''\n",
    "        Takes the arguments:\n",
    "            - cat, string data type, category(es) name\n",
    "        Computes n highest terms TF-IDF scores for each essay feature entered category(es).\n",
    "        Sums the scores by term.\n",
    "        Returns:\n",
    "            - a DataFrame of the summed scores with the associated terms\n",
    "            - a DataFrame containing only the associated terms\n",
    "        '''\n",
    "        df_scores = pd.DataFrame()\n",
    "        terms_tfidf = pd.DataFrame()\n",
    "        for name in essay_names:\n",
    "            # Initializes variable to class, empty score \n",
    "            # The max_features returns the n highest TF-IDF scores\n",
    "            word_embeddings = gensim.models.Word2Vec(all_sentences, size=96, window=5, min_count=1, workers=2, sg=1)\n",
    "            # Fits/transforms training data and returns a score matrix \n",
    "            tfidf_scores = vectorizer.fit_transform(profiles_nlp[f'{name}_{cat}'])\n",
    "            # Gets vocabulary of terms\n",
    "            feature_names = vectorizer.get_feature_names()\n",
    "            # Creates a DataFrame of the sum of all the n highest profile essays TF-IDF score \n",
    "            sum_tfidf = pd.DataFrame(tfidf_scores.T.todense(), index=feature_names) \\\n",
    "                                            .T \\\n",
    "                                            .sum() \\\n",
    "                                            .astype(int) \\\n",
    "                                            .sort_values(ascending = False) \\\n",
    "                                            .to_frame() \\\n",
    "                                            .reset_index() \\\n",
    "                                            .rename(columns={'index':'Terms', 0: 'TF-IDF'})\n",
    "            # Adds the summed scores with the associated terms, tuples, to DataFrame \n",
    "            df_scores[f'{name}_{cat[20:]}'] = tuple(zip(sum_tfidf['Terms'], sum_tfidf['TF-IDF']))\n",
    "            # Adds only the terms to DataFrame\n",
    "            terms_tfidf[f'{name}_{cat[20:]}'] = df_scores[f'{name}_{cat[20:]}'].apply(lambda x: x[0])    \n",
    "        # Names DataFrame\n",
    "        df_scores.name = f'essays_{cat[20:]}_scores'\n",
    "        terms_tfidf.name = f'essays_{cat[20:]}_terms'\n",
    "        return df_scores, terms_tfidf\n",
    "    # -------------------------------------------------------------------------------------------------------------------------------- Class methods\n",
    "    # ----------------------------------------------------------------------------------- Saving TF_IDF results\n",
    "    def save_scores(self):\n",
    "        '''\n",
    "        Saves all the TF-IDF score results by element category(es) \n",
    "        '''\n",
    "        for df in self.tfidf_scores:\n",
    "            df.to_csv(f'data/tfidf/{df.name}.csv')\n",
    "    #\n",
    "    def save_terms(self):\n",
    "        '''\n",
    "        Saves all the TF-IDF terms results by element category(es) \n",
    "        '''\n",
    "        for df in self.tfidf_scores:\n",
    "            df.to_csv(f'data/tfidf/{df.name}.csv')\n",
    "    # ----------------------------------------------------------------------------------- Displaying TF_IDF results\n",
    "    def display_scores_dfs(self):\n",
    "        '''\n",
    "        Displays all the TF-IDF scores DataFrames by element category(es) \n",
    "        '''\n",
    "        for df in self.tfidf_scores:\n",
    "            display(df.style.set_properties(**{'text-align': 'center'}))\n",
    "    #\n",
    "    def display_terms_dfs(self):\n",
    "        '''\n",
    "        Displays all the TF-IDF terms DataFrames by element category(es) \n",
    "        '''\n",
    "        for df in self.tfidf_terms:\n",
    "            display(df.style.set_properties(**{'text-align': 'center'}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
